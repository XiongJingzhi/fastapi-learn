# Level 4 å®šæ—¶ä»»åŠ¡æ¨¡å—å®æ–½è®¡åˆ’

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**ç›®æ ‡:** åœ¨Level 4ï¼ˆç”Ÿäº§å°±ç»ªï¼‰ä¸­è¡¥å……å®šæ—¶ä»»åŠ¡æ¨¡å—ï¼Œæä¾›ä»åŸºç¡€åˆ°ç”Ÿäº§çº§çš„å®Œæ•´å­¦ä¹ è·¯å¾„

**æ¶æ„:** é‡‡ç”¨æŠ€æœ¯æ ˆåˆ†ç¦»æ–¹æ¡ˆï¼Œåˆ›å»º6ä¸ªå­¦ä¹ ç¬”è®°æ–‡ä»¶ï¼ˆæ€»è§ˆ+å¯¹æ¯”+æœ€ä½³å®è·µï¼‰å’Œ12ä¸ªæ¸è¿›å¼ä»£ç ç¤ºä¾‹ï¼ˆLevel 1-4ï¼‰ï¼Œæ¶µç›–APSchedulerå’ŒCelery Beatä¸¤ç§æŠ€æœ¯æ ˆ

**æŠ€æœ¯æ ˆ:** APScheduler 3.10+, Celery 5.3+, Redis 5.0+, FastAPI, SQLAlchemy

---

## Task 1: åˆ›å»ºç¤ºä¾‹ç›®å½•ç»“æ„

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/`
- Create: `study/level4/examples/07_scheduled_tasks/apscheduler/`
- Create: `study/level4/examples/07_scheduled_tasks/celery_beat/`

**Step 1: åˆ›å»ºç›®å½•**

```bash
mkdir -p study/level4/examples/07_scheduled_tasks/apscheduler
mkdir -p study/level4/examples/07_scheduled_tasks/celery_beat
```

**Step 2: éªŒè¯ç›®å½•åˆ›å»º**

```bash
ls -la study/level4/examples/07_scheduled_tasks/
```

Expected: çœ‹åˆ° `apscheduler/` å’Œ `celery_beat/` ç›®å½•

**Step 3: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/
git commit -m "feat: create scheduled tasks examples directory structure"
```

---

## Task 2: åˆ›å»ºLevel 1ç¤ºä¾‹ - ç®€å•å®šæ—¶å™¨

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/level1_simple_timer.py`

**Step 1: åˆ›å»ºLevel 1ç¤ºä¾‹æ–‡ä»¶**

```python
"""
Level 1: æœ€ç®€å•çš„å®šæ—¶ä»»åŠ¡

å­¦ä¹ ç›®æ ‡ï¼š
- ç†è§£ScheduleråŸºæœ¬æ¦‚å¿µ
- è¿è¡Œä½ çš„ç¬¬ä¸€ä¸ªå®šæ—¶ä»»åŠ¡
- çœ‹åˆ°å®é™…æ•ˆæœ

è¿è¡Œï¼špython level1_simple_timer.py
é¢„æœŸï¼šæ¯3ç§’æ‰“å°ä¸€æ¬¡"Hello, Scheduler!"
"""

from apscheduler.schedulers.blocking import BlockingScheduler
from datetime import datetime, timedelta

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = BlockingScheduler()

# å®šä¹‰ä»»åŠ¡
def print_hello():
    print(f"[{datetime.now()}] Hello, Scheduler!")

# æ·»åŠ ä»»åŠ¡ï¼šæ¯3ç§’æ‰§è¡Œä¸€æ¬¡
scheduler.add_job(print_hello, 'interval', seconds=3)

# æ·»åŠ ä»»åŠ¡ï¼š5ç§’åæ‰§è¡Œä¸€æ¬¡
scheduler.add_job(
    lambda: print(f"[{datetime.now()}] One-time task!"),
    'date',
    run_date=datetime.now() + timedelta(seconds=5)
)

print("Scheduler started. Press Ctrl+C to exit.")

# å¯åŠ¨è°ƒåº¦å™¨ï¼ˆé˜»å¡ï¼‰
try:
    scheduler.start()
except (KeyboardInterrupt, SystemExit):
    print("Scheduler stopped.")
```

**Step 2: åˆ›å»º__init__.py**

```bash
touch study/level4/examples/07_scheduled_tasks/__init__.py
```

**Step 3: æµ‹è¯•è¿è¡Œ**

```bash
cd study/level4/examples/07_scheduled_tasks
timeout 15 python level1_simple_timer.py
```

Expected: çœ‹åˆ° "Hello, Scheduler!" æ‰“å°çº¦5æ¬¡

**Step 4: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/
git commit -m "feat: add Level 1 simple timer example"
```

---

## Task 3: åˆ›å»ºLevel 2ç¤ºä¾‹ - æ•°æ®æ¸…ç†ä»»åŠ¡

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/level2_data_cleanup.py`

**Step 1: åˆ›å»ºLevel 2ç¤ºä¾‹æ–‡ä»¶**

```python
"""
Level 2: æ•°æ®æ¸…ç†ä»»åŠ¡

å®é™…åœºæ™¯ï¼š
- æ¯å°æ—¶æ¸…ç†è¿‡æœŸtoken
- æ¯å¤©å‡Œæ™¨2ç‚¹å½’æ¡£æ—¥å¿—
- ä»»åŠ¡å¤±è´¥è‡ªåŠ¨é‡è¯•

è¿è¡Œï¼špython level2_data_cleanup.py
"""

import asyncio
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DataCleanupService:
    """æ•°æ®æ¸…ç†æœåŠ¡"""

    def __init__(self):
        self.scheduler = AsyncIOScheduler()

    async def cleanup_expired_tokens(self):
        """æ¸…ç†è¿‡æœŸtokenï¼ˆLevel 2ç¤ºä¾‹ï¼‰"""
        try:
            logger.info("Starting token cleanup...")

            # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œ
            expired_tokens = await self._get_expired_tokens()
            logger.info(f"Found {len(expired_tokens)} expired tokens")

            # åˆ é™¤è¿‡æœŸtoken
            deleted = await self._delete_tokens(expired_tokens)

            logger.info(f"Cleanup completed: {deleted} tokens deleted")

        except Exception as e:
            logger.error(f"Token cleanup failed: {e}")
            raise

    async def archive_logs(self):
        """å½’æ¡£æ—¥å¿—ï¼ˆæ¯å¤©å‡Œæ™¨2ç‚¹ï¼‰"""
        try:
            logger.info("Starting log archival...")

            # æ¨¡æ‹Ÿå½’æ¡£æ“ä½œ
            await self._compress_logs()
            await self._upload_to_storage()

            logger.info("Log archival completed")

        except Exception as e:
            logger.error(f"Log archival failed: {e}")
            raise

    async def _get_expired_tokens(self):
        """æ¨¡æ‹Ÿï¼šè·å–è¿‡æœŸtoken"""
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        return list(range(10))  # æ¨¡æ‹Ÿ10ä¸ªè¿‡æœŸtoken

    async def _delete_tokens(self, tokens):
        """æ¨¡æ‹Ÿï¼šåˆ é™¤token"""
        await asyncio.sleep(0.2)
        return len(tokens)

    async def _compress_logs(self):
        """æ¨¡æ‹Ÿï¼šå‹ç¼©æ—¥å¿—"""
        await asyncio.sleep(1)

    async def _upload_to_storage(self):
        """æ¨¡æ‹Ÿï¼šä¸Šä¼ å­˜å‚¨"""
        await asyncio.sleep(2)

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        # æ¯å°æ—¶æ¸…ç†è¿‡æœŸtoken
        self.scheduler.add_job(
            self.cleanup_expired_tokens,
            'interval',
            hours=1,
            id='cleanup_tokens',
            max_instances=1,  # é˜²æ­¢ä»»åŠ¡é‡å 
            misfire_grace_time=300  # å®¹å¿5ç§’å»¶è¿Ÿ
        )

        # æ¯å¤©å‡Œæ™¨2ç‚¹å½’æ¡£æ—¥å¿—
        self.scheduler.add_job(
            self.archive_logs,
            'cron',
            hour=2,
            minute=0,
            id='archive_logs',
            max_instances=1
        )

        # æµ‹è¯•ç”¨ï¼šæ¯30ç§’æ‰§è¡Œä¸€æ¬¡
        self.scheduler.add_job(
            self.cleanup_expired_tokens,
            'interval',
            seconds=30,
            id='test_cleanup'
        )

        self.scheduler.start()
        logger.info("Data cleanup service started")

# è¿è¡Œ
if __name__ == "__main__":
    service = DataCleanupService()
    service.start()

    try:
        asyncio.Event().wait()  # ä¿æŒè¿è¡Œ
    except KeyboardInterrupt:
        logger.info("Shutting down...")
        service.scheduler.shutdown()
```

**Step 2: æµ‹è¯•è¿è¡Œ**

```bash
cd study/level4/examples/07_scheduled_tasks
timeout 40 python level2_data_cleanup.py
```

Expected: çœ‹åˆ°ä»»åŠ¡åœ¨30ç§’æ—¶æ‰§è¡Œ

**Step 3: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/level2_data_cleanup.py
git commit -m "feat: add Level 2 data cleanup example"
```

---

## Task 4: åˆ›å»ºLevel 3ç¤ºä¾‹ - åˆ†å¸ƒå¼åè°ƒå™¨

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/level3_distributed_coordinator.py`

**Step 1: åˆ›å»ºLevel 3ç¤ºä¾‹æ–‡ä»¶**

```python
"""
Level 3: åˆ†å¸ƒå¼ä»»åŠ¡åè°ƒ

é—®é¢˜ï¼šå¤šå®ä¾‹éƒ¨ç½²æ—¶ï¼Œä»»åŠ¡ä¼šé‡å¤æ‰§è¡Œ
æ–¹æ¡ˆï¼šä½¿ç”¨Redisåˆ†å¸ƒå¼é”

è¿è¡Œï¼š
- ç»ˆç«¯1ï¼špython level3_distributed_coordinator.py --instance-id=node1
- ç»ˆç«¯2ï¼špython level3_distributed_coordinator.py --instance-id=node2
è§‚å¯Ÿï¼šåªæœ‰ä¸€ä¸ªå®ä¾‹æ‰§è¡Œä»»åŠ¡
"""

import asyncio
import argparse
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class DistributedScheduler:
    """åˆ†å¸ƒå¼ä»»åŠ¡è°ƒåº¦å™¨ï¼ˆæ¨¡æ‹Ÿç‰ˆæœ¬ï¼‰"""

    def __init__(self, instance_id: str):
        self.instance_id = instance_id
        self.scheduler = AsyncIOScheduler()
        # æ¨¡æ‹ŸRedisé”
        self.lock_acquired = False

    async def distributed_task(self):
        """åˆ†å¸ƒå¼ä»»åŠ¡ï¼ˆåªåœ¨ä¸€ä¸ªå®ä¾‹ä¸Šæ‰§è¡Œï¼‰"""

        # æ¨¡æ‹Ÿè·å–åˆ†å¸ƒå¼é”
        lock_key = "lock:monthly_report"

        # ç®€åŒ–ç‰ˆï¼šç¬¬ä¸€ä¸ªå®ä¾‹è·å–é”
        if not self.lock_acquired and self.instance_id == "node1":
            self.lock_acquired = True
            logger.info(f"[{self.instance_id}] Lock acquired, starting task...")

            # æ‰§è¡Œä»»åŠ¡ï¼ˆæ¨¡æ‹ŸæœˆæŠ¥ç”Ÿæˆï¼‰
            await self._generate_monthly_report()

            logger.info(f"[{self.instance_id}] Task completed")
            self.lock_acquired = False
        else:
            logger.info(f"[{self.instance_id}] Another instance is running the task")

    async def _generate_monthly_report(self):
        """ç”ŸæˆæœˆæŠ¥ï¼ˆæ¨¡æ‹Ÿè€—æ—¶æ“ä½œï¼‰"""
        logger.info("Generating monthly report...")
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿ2ç§’æ“ä½œ
        logger.info("Monthly report generated")

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        # æµ‹è¯•ç”¨ï¼šæ¯15ç§’æ‰§è¡Œä¸€æ¬¡
        self.scheduler.add_job(
            self.distributed_task,
            'interval',
            seconds=15,
            id='test_distributed_task'
        )

        self.scheduler.start()
        logger.info(f"[{self.instance_id}] Distributed scheduler started")

# è¿è¡Œ
if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - [%(levelname)s] - %(message)s'
    )

    parser = argparse.ArgumentParser()
    parser.add_argument("--instance-id", default="node1")
    args = parser.parse_args()

    coordinator = DistributedScheduler(args.instance_id)
    coordinator.start()

    try:
        asyncio.Event().wait()
    except KeyboardInterrupt:
        logger.info(f"[{args.instance_id}] Shutting down...")
        coordinator.scheduler.shutdown()
```

**Step 2: æµ‹è¯•è¿è¡Œï¼ˆå•å®ä¾‹ï¼‰**

```bash
cd study/level4/examples/07_scheduled_tasks
timeout 40 python level3_distributed_coordinator.py
```

**Step 3: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/level3_distributed_coordinator.py
git commit -m "feat: add Level 3 distributed coordinator example"
```

---

## Task 5: åˆ›å»ºLevel 4ç¤ºä¾‹ - ç”Ÿäº§ç›‘æ§ç³»ç»Ÿ

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/level4_production_monitor.py`

**Step 1: åˆ›å»ºLevel 4ç¤ºä¾‹æ–‡ä»¶**

```python
"""
Level 4: ç”Ÿäº§çº§å®šæ—¶ä»»åŠ¡ç›‘æ§

åŠŸèƒ½ï¼š
- ä»»åŠ¡æ‰§è¡Œå†å²è®°å½•
- ä»»åŠ¡çŠ¶æ€ç›‘æ§
- Webç®¡ç†ç•Œé¢
- å¤±è´¥å‘Šè­¦

è¿è¡Œï¼špython level4_production_monitor.py
è®¿é—®ï¼šhttp://localhost:8000/docs
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import AsyncIOExecutor
from datetime import datetime
import uvicorn
from typing import List, Optional
import logging
import asyncio

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ•°æ®æ¨¡å‹
class TaskExecution(BaseModel):
    """ä»»åŠ¡æ‰§è¡Œè®°å½•"""
    task_id: str
    status: str
    started_at: datetime
    completed_at: Optional[datetime] = None
    duration: Optional[float] = None
    error: Optional[str] = None

class TaskStatus(BaseModel):
    """ä»»åŠ¡çŠ¶æ€"""
    task_id: str
    next_run_time: Optional[datetime]
    trigger: str
    executions: List[TaskExecution]

# ç”Ÿäº§çº§è°ƒåº¦å™¨
class ProductionScheduler:
    """ç”Ÿäº§çº§å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""

    def __init__(self, db_url: str = "sqlite:///tasks.db"):
        jobstores = {
            'default': SQLAlchemyJobStore(url=db_url)
        }
        executors = {
            'default': AsyncIOExecutor(max_workers=10)
        }

        self.scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            executors=executors,
            job_defaults={
                'coalesce': True,
                'max_instances': 1,
                'misfire_grace_time': 300
            }
        )

        self.execution_history: dict = {}

    async def execute_with_monitoring(
        self,
        task_id: str,
        func,
        *args,
        **kwargs
    ):
        """æ‰§è¡Œä»»åŠ¡å¹¶è®°å½•ç›‘æ§æ•°æ®"""
        execution = TaskExecution(
            task_id=task_id,
            status="running",
            started_at=datetime.now()
        )

        if task_id not in self.execution_history:
            self.execution_history[task_id] = []

        self.execution_history[task_id].append(execution)

        try:
            result = await func(*args, **kwargs)

            execution.status = "success"
            execution.completed_at = datetime.now()
            execution.duration = (
                execution.completed_at - execution.started_at
            ).total_seconds()

            logger.info(
                f"Task {task_id} completed in {execution.duration:.2f}s"
            )

            return result

        except Exception as e:
            execution.status = "failed"
            execution.completed_at = datetime.now()
            execution.duration = (
                execution.completed_at - execution.started_at
            ).total_seconds()
            execution.error = str(e)

            logger.error(f"Task {task_id} failed: {e}")
            raise

    def get_task_status(self, task_id: str) -> TaskStatus:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        job = self.scheduler.get_job(task_id)

        if not job:
            raise HTTPException(status_code=404, detail="Task not found")

        return TaskStatus(
            task_id=task_id,
            next_run_time=job.next_run_time,
            trigger=str(job.trigger),
            executions=self.execution_history.get(task_id, [])
        )

    def list_tasks(self) -> List[dict]:
        """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
        jobs = self.scheduler.get_jobs()
        return [
            {
                "id": job.id,
                "name": job.name,
                "next_run_time": str(job.next_run_time),
                "trigger": str(job.trigger)
            }
            for job in jobs
        ]

    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.scheduler.start()
        logger.info("Production scheduler started")

# FastAPIåº”ç”¨
app = FastAPI(title="å®šæ—¶ä»»åŠ¡ç›‘æ§ç³»ç»Ÿ")
scheduler = ProductionScheduler()

# ç¤ºä¾‹ä»»åŠ¡
async def data_report_task():
    """æ•°æ®æŠ¥è¡¨ä»»åŠ¡"""
    logger.info("Generating data report...")
    await asyncio.sleep(1)
    logger.info("Data report generated")

async def cache_warmup_task():
    """ç¼“å­˜é¢„çƒ­ä»»åŠ¡"""
    logger.info("Warming up cache...")
    await asyncio.sleep(0.5)
    logger.info("Cache warmed up")

# Endpoints
@app.get("/")
async def root():
    return {
        "message": "å®šæ—¶ä»»åŠ¡ç›‘æ§ç³»ç»Ÿ",
        "docs": "/docs",
        "tasks": len(scheduler.list_tasks())
    }

@app.get("/api/tasks")
async def list_tasks():
    return scheduler.list_tasks()

@app.get("/api/tasks/{task_id}")
async def get_task_status(task_id: str):
    return scheduler.get_task_status(task_id)

@app.delete("/api/tasks/{task_id}")
async def delete_task(task_id: str):
    scheduler.scheduler.remove_job(task_id)
    return {"message": f"Task {task_id} deleted"}

# åº”ç”¨ç”Ÿå‘½å‘¨æœŸ
@app.on_event("startup")
async def startup_event():
    scheduler.scheduler.add_job(
        lambda: scheduler.execute_with_monitoring(
            "data_report",
            data_report_task
        ),
        'interval',
        seconds=30,
        id='data_report',
        name='æ•°æ®æŠ¥è¡¨'
    )

    scheduler.scheduler.add_job(
        lambda: scheduler.execute_with_monitoring(
            "cache_warmup",
            cache_warmup_task
        ),
        'interval',
        seconds=45,
        id='cache_warmup',
        name='ç¼“å­˜é¢„çƒ­'
    )

    scheduler.start()

@app.on_event("shutdown")
async def shutdown_event():
    scheduler.scheduler.shutdown()

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Step 2: æµ‹è¯•APIå¯åŠ¨**

```bash
cd study/level4/examples/07_scheduled_tasks
timeout 10 python level4_production_monitor.py &
sleep 3
curl http://localhost:8000/api/tasks
```

**Step 3: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/level4_production_monitor.py
git commit -m "feat: add Level 4 production monitor example"
```

---

## Task 6: åˆ›å»ºAPScheduleråµŒå…¥ç¤ºä¾‹

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/apscheduler/embedded_app.py`

**Step 1: åˆ›å»ºåµŒå…¥æ¨¡å¼ç¤ºä¾‹**

```python
"""
APScheduleråµŒå…¥FastAPIç¤ºä¾‹

å±•ç¤ºå¦‚ä½•åœ¨FastAPIåº”ç”¨ä¸­åµŒå…¥è°ƒåº¦å™¨
"""

from contextlib import asynccontextmanager
from fastapi import FastAPI
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import logging

logger = logging.getLogger(__name__)

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = AsyncIOScheduler()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶
    logger.info("Starting scheduler...")
    scheduler.start()
    yield
    # å…³é—­æ—¶
    logger.info("Shutting down scheduler...")
    scheduler.shutdown()

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(lifespan=lifespan)

# å®šä¹‰ä»»åŠ¡
async def health_check_task():
    """å¥åº·æ£€æŸ¥ä»»åŠ¡"""
    logger.info("Running health check...")

# æ·»åŠ å®šæ—¶ä»»åŠ¡
scheduler.add_job(
    health_check_task,
    'interval',
    minutes=5,
    id='health_check'
)

@app.get("/")
async def root():
    return {"message": "FastAPI with embedded APScheduler"}

@app.get("/scheduler/status")
async def scheduler_status():
    jobs = scheduler.get_jobs()
    return {
        "status": "running" if scheduler.running else "stopped",
        "jobs": [
            {"id": job.id, "name": job.name}
            for job in jobs
        ]
    }
```

**Step 2: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/apscheduler/embedded_app.py
git commit -m "feat: add APScheduler embedded app example"
```

---

## Task 7: åˆ›å»ºAPSchedulerç‹¬ç«‹è¿›ç¨‹ç¤ºä¾‹

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/apscheduler/standalone_app.py`
- Create: `study/level4/examples/07_scheduled_tasks/apscheduler/config.py`

**Step 1: åˆ›å»ºé…ç½®æ–‡ä»¶**

```python
"""
APScheduleré…ç½®
"""

from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from apscheduler.executors.pool import ThreadPoolExecutor

# JobStoreé…ç½®
JOBSTORES = {
    'default': SQLAlchemyJobStore(url='sqlite:///jobs.db')
}

# Executoré…ç½®
EXECUTORS = {
    'default': ThreadPoolExecutor(max_workers=20)
}

# ä»»åŠ¡é»˜è®¤é…ç½®
JOB_DEFAULTS = {
    'coalesce': True,
    'max_instances': 3,
    'misfire_grace_time': 60
}

# è°ƒåº¦å™¨é…ç½®
SCHEDULER_CONFIG = {
    'jobstores': JOBSTORES,
    'executors': EXECUTORS,
    'job_defaults': JOB_DEFAULTS,
    'timezone': 'UTC'
}
```

**Step 2: åˆ›å»ºç‹¬ç«‹åº”ç”¨æ–‡ä»¶**

```python
"""
APSchedulerç‹¬ç«‹è¿›ç¨‹ç¤ºä¾‹

é€‚åˆç”Ÿäº§ç¯å¢ƒçš„ç‹¬ç«‹è°ƒåº¦å™¨è¿›ç¨‹
"""

from apscheduler.schedulers.blocking import BlockingScheduler
from config import SCHEDULER_CONFIG
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = BlockingScheduler(**SCHEDULER_CONFIG)

# å®šä¹‰ä»»åŠ¡
def task1():
    logger.info("Executing task1...")

def task2():
    logger.info("Executing task2...")

# æ·»åŠ ä»»åŠ¡
scheduler.add_job(
    task1,
    'interval',
    minutes=10,
    id='task1',
    name='Periodic Task 1'
)

scheduler.add_job(
    task2,
    'cron',
    hour='*/2',
    id='task2',
    name='Periodic Task 2'
)

if __name__ == "__main__":
    try:
        logger.info("Starting standalone scheduler...")
        scheduler.start()
    except (KeyboardInterrupt, SystemExit):
        logger.info("Shutting down scheduler...")
        scheduler.shutdown()
```

**Step 3: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/apscheduler/
git commit -m "feat: add APScheduler standalone app example"
```

---

## Task 8: åˆ›å»ºCelery BeatåŸºç¡€ç¤ºä¾‹

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/celery_beat/tasks.py`
- Create: `study/level4/examples/07_scheduled_tasks/celery_beat/beat_config.py`

**Step 1: åˆ›å»ºä»»åŠ¡å®šä¹‰æ–‡ä»¶**

```python
"""
Celeryä»»åŠ¡å®šä¹‰
"""

from celery import Celery
import logging

logger = logging.getLogger(__name__)

# åˆ›å»ºCeleryåº”ç”¨
app = Celery('tasks')

# é…ç½®
app.conf.update(
    broker_url='redis://localhost:6379/0',
    result_backend='redis://localhost:6379/1',
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
)

# å®šä¹‰ä»»åŠ¡
@app.task
def add(x, y):
    """ç®€å•åŠ æ³•ä»»åŠ¡"""
    result = x + y
    logger.info(f"Adding {x} + {y} = {result}")
    return result

@app.task
def send_email(to, subject, body):
    """å‘é€é‚®ä»¶ä»»åŠ¡"""
    logger.info(f"Sending email to {to}: {subject}")
    # æ¨¡æ‹Ÿå‘é€é‚®ä»¶
    import time
    time.sleep(1)
    logger.info(f"Email sent to {to}")
    return {"status": "sent", "to": to}

@app.task
def generate_report(report_type):
    """ç”ŸæˆæŠ¥è¡¨ä»»åŠ¡"""
    logger.info(f"Generating {report_type} report...")
    import time
    time.sleep(2)
    logger.info(f"{report_type} report generated")
    return {"report": report_type, "status": "completed"}
```

**Step 2: åˆ›å»ºBeaté…ç½®æ–‡ä»¶**

```python
"""
Celery Beatè°ƒåº¦é…ç½®
"""

from celery.schedules import crontab

# Beatè°ƒåº¦é…ç½®
beat_schedule = {
    'add-every-30-seconds': {
        'task': 'tasks.add',
        'schedule': 30.0,
        'args': (16, 16)
    },
    'send-daily-report': {
        'task': 'tasks.send_email',
        'schedule': crontab(hour=18, minute=0),
        'args': ("admin@example.com", "Daily Report", "Report content")
    },
    'generate-weekly-report': {
        'task': 'tasks.generate_report',
        'schedule': crontab(day_of_week=1, hour=8, minute=0),
        'args': ("weekly",)
    },
}
```

**Step 3: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/celery_beat/tasks.py
git add study/level4/examples/07_scheduled_tasks/celery_beat/beat_config.py
git commit -m "feat: add Celery Beat basic example"
```

---

## Task 9: åˆ›å»ºCeleryç‹¬ç«‹Workerç¤ºä¾‹

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/celery_beat/standalone_worker.py`

**Step 1: åˆ›å»ºWorkeræ–‡ä»¶**

```python
"""
Celeryç‹¬ç«‹Workerç¤ºä¾‹

å¯åŠ¨å‘½ä»¤ï¼š
  celery -A standalone_worker worker --loglevel=info

å¯åŠ¨Beatï¼š
  celery -A standalone_worker beat --loglevel=info
"""

from tasks import app, beat_schedule

# æ·»åŠ beaté…ç½®
app.conf.beat_schedule = beat_schedule

if __name__ == "__main__":
    app.start()
```

**Step 2: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/celery_beat/standalone_worker.py
git commit -m "feat: add Celery standalone worker example"
```

---

## Task 10: åˆ›å»ºç¤ºä¾‹å¯¼èˆªREADME

**Files:**
- Create: `study/level4/examples/07_scheduled_tasks/README.md`

**Step 1: åˆ›å»ºREADMEæ–‡ä»¶**

```markdown
# å®šæ—¶ä»»åŠ¡ç¤ºä¾‹å¯¼èˆª

## ğŸ¯ æŒ‰éš¾åº¦æ¸è¿›å­¦ä¹ 

### Level 1: Hello World
- `level1_simple_timer.py` - æœ€ç®€å•çš„å®šæ—¶ä»»åŠ¡
  - 5ç§’åæ‰§è¡Œä¸€æ¬¡
  - æ¯3ç§’æ‰“å°"Hello"
  - ä»£ç é‡ï¼š20è¡Œ

### Level 2: å®é™…åœºæ™¯
- `level2_data_cleanup.py` - æ•°æ®æ¸…ç†ä»»åŠ¡
  - åˆ é™¤è¿‡æœŸtoken
  - é”™è¯¯å¤„ç†
  - ä»£ç é‡ï¼š80è¡Œ

### Level 3: åˆ†å¸ƒå¼åè°ƒ
- `level3_distributed_coordinator.py` - å¤šå®ä¾‹åªæ‰§è¡Œä¸€æ¬¡
  - åˆ†å¸ƒå¼é”ï¼ˆæ¨¡æ‹Ÿï¼‰
  - ä»»åŠ¡åè°ƒ
  - ä»£ç é‡ï¼š100è¡Œ

### Level 4: ç”Ÿäº§ç›‘æ§
- `level4_production_monitor.py` - å®Œæ•´ç”Ÿäº§çº§æ–¹æ¡ˆ
  - ä»»åŠ¡ç›‘æ§
  - æ‰§è¡Œå†å²
  - Webç®¡ç†ç•Œé¢
  - ä»£ç é‡ï¼š300è¡Œ

## ğŸ› ï¸ æŠ€æœ¯æ ˆå®Œæ•´ç¤ºä¾‹

### APSchedulerå®Œæ•´å®ç°
- `apscheduler/embedded_app.py` - åµŒå…¥FastAPI
- `apscheduler/standalone_app.py` - ç‹¬ç«‹è¿›ç¨‹
- `apscheduler/config.py` - é…ç½®ç®¡ç†

### Celery Beatå®Œæ•´å®ç°
- `celery_beat/tasks.py` - ä»»åŠ¡å®šä¹‰
- `celery_beat/beat_config.py` - Beaté…ç½®
- `celery_beat/standalone_worker.py` - ç‹¬ç«‹worker

## ğŸš€ å¿«é€Ÿè¿è¡Œ

### Level 1ç¤ºä¾‹
```bash
cd study/level4/examples/07_scheduled_tasks
python level1_simple_timer.py
```

### Level 2ç¤ºä¾‹
```bash
python level2_data_cleanup.py
```

### Level 4ç¤ºä¾‹ï¼ˆéœ€è¦FastAPIï¼‰
```bash
pip install fastapi uvicorn
python level4_production_monitor.py
# è®¿é—® http://localhost:8000/docs
```

### Celeryç¤ºä¾‹ï¼ˆéœ€è¦Redisï¼‰
```bash
# å¯åŠ¨Redis
docker run -d -p 6379:6379 redis:alpine

# å¯åŠ¨Worker
cd celery_beat
celery -A standalone_worker worker --loglevel=info

# å¯åŠ¨Beatï¼ˆå¦ä¸€ä¸ªç»ˆç«¯ï¼‰
celery -A standalone_worker beat --loglevel=info
```

## ğŸ“¦ ä¾èµ–å®‰è£…

```bash
# åŸºç¡€ä¾èµ–
pip install apscheduler

# FastAPIä¾èµ–
pip install fastapi uvicorn

# Celeryä¾èµ–
pip install celery redis
```
```

**Step 2: Commit**

```bash
git add study/level4/examples/07_scheduled_tasks/README.md
git commit -m "docs: add scheduled tasks examples README"
```

---

## Task 11: åˆ›å»ºæ€»è§ˆç¬”è®°æ–‡ä»¶

**Files:**
- Create: `study/level4/notes/07_scheduled_tasks.md`

**Step 1: åˆ›å»ºæ€»è§ˆç¬”è®°**

å†…å®¹åŒ…å«ï¼š
- å®šæ—¶ä»»åŠ¡åœ¨æ¶æ„ä¸­çš„ä½ç½®
- APScheduler vs Celery Beatå¯¹æ¯”è¡¨
- å­¦ä¹ è·¯å¾„å¯¼èˆª
- å¿«é€Ÿé€‰æ‹©æŒ‡å—

ï¼ˆè¯¦ç»†å†…å®¹è§è®¾è®¡æ–‡æ¡£ç¬¬3.1èŠ‚ï¼‰

**Step 2: Commit**

```bash
git add study/level4/notes/07_scheduled_tasks.md
git commit -m "docs: add scheduled tasks overview notes"
```

---

## Task 12: åˆ›å»ºAPScheduleråŸºç¡€ç¬”è®°

**Files:**
- Create: `study/level4/notes/07a_ap_scheduler_intro.md`

**Step 1: åˆ›å»ºAPScheduleråŸºç¡€ç¬”è®°**

å†…å®¹åŒ…å«ï¼š
- åŸºæœ¬æ¦‚å¿µï¼ˆè´¹æ›¼æŠ€å·§ç±»æ¯”ï¼‰
- ä¸‰ç§Triggerè¯¦è§£
- Hello Worldç¤ºä¾‹
- ä¸FastAPIé›†æˆ
- å°å®éªŒ

**Step 2: Commit**

```bash
git add study/level4/notes/07a_ap_scheduler_intro.md
git commit -m "docs: add APScheduler intro notes"
```

---

## Task 13: åˆ›å»ºAPScheduleré«˜çº§ç¬”è®°

**Files:**
- Create: `study/level4/notes/07b_ap_scheduler_advanced.md`

**Step 1: åˆ›å»ºAPScheduleré«˜çº§ç¬”è®°**

å†…å®¹åŒ…å«ï¼š
- å®é™…åœºæ™¯ï¼ˆæ•°æ®æ¸…ç†ã€ç¼“å­˜é¢„çƒ­ï¼‰
- ä»»åŠ¡æŒä¹…åŒ–
- ä»»åŠ¡ç®¡ç†
- é”™è¯¯å¤„ç†å’Œé‡è¯•

**Step 2: Commit**

```bash
git add study/level4/notes/07b_ap_scheduler_advanced.md
git commit -m "docs: add APScheduler advanced notes"
```

---

## Task 14: åˆ›å»ºCelery BeatåŸºç¡€ç¬”è®°

**Files:**
- Create: `study/level4/notes/07c_celery_beat_intro.md`

**Step 1: åˆ›å»ºCelery BeatåŸºç¡€ç¬”è®°**

å†…å®¹åŒ…å«ï¼š
- ä¸ºä»€ä¹ˆéœ€è¦Celery Beat
- Celeryæ¶æ„ï¼ˆç”Ÿæ´»ç±»æ¯”ï¼‰
- Hello Worldç¤ºä¾‹
- å¸¸ç”¨Scheduleé…ç½®

**Step 2: Commit**

```bash
git add study/level4/notes/07c_celery_beat_intro.md
git commit -m "docs: add Celery Beat intro notes"
```

---

## Task 15: åˆ›å»ºCelery Beaté«˜çº§ç¬”è®°

**Files:**
- Create: `study/level4/notes/07d_celery_beat_advanced.md`

**Step 1: åˆ›å»ºCelery Beaté«˜çº§ç¬”è®°**

å†…å®¹åŒ…å«ï¼š
- åˆ†å¸ƒå¼ä»»åŠ¡
- é«˜çº§ç‰¹æ€§ï¼ˆchainã€groupã€chordï¼‰
- ä»»åŠ¡ç›‘æ§ï¼ˆFlowerï¼‰
- å¤±è´¥å¤„ç†

**Step 2: Commit**

```bash
git add study/level4/notes/07d_celery_beat_advanced.md
git commit -m "docs: add Celery Beat advanced notes"
```

---

## Task 16: åˆ›å»ºæœ€ä½³å®è·µç¬”è®°

**Files:**
- Create: `study/level4/notes/07e_best_practices.md`

**Step 1: åˆ›å»ºæœ€ä½³å®è·µç¬”è®°**

å†…å®¹åŒ…å«ï¼š
- æŠ€æœ¯é€‰å‹å†³ç­–æ ‘
- æ¶æ„æ¨¡å¼å¯¹æ¯”
- ç”Ÿäº§ç¯å¢ƒæ¸…å•
- å¸¸è§é™·é˜±
- å®æˆ˜æ¡ˆä¾‹

**Step 2: Commit**

```bash
git add study/level4/notes/07e_best_practices.md
git commit -m "docs: add scheduled tasks best practices"
```

---

## Task 17: æ›´æ–°Level 4 README

**Files:**
- Modify: `study/level4/README.md`

**Step 1: è¯»å–ç°æœ‰README**

```bash
cat study/level4/README.md
```

**Step 2: æ·»åŠ å®šæ—¶ä»»åŠ¡ä¸»é¢˜**

åœ¨"ä¸»é¢˜ 5: é™æµã€ç†”æ–­ã€é™çº§"ä¹‹åæ·»åŠ ï¼š

```markdown
---

### ä¸»é¢˜ 7ï¼šå®šæ—¶ä»»åŠ¡

**ä¸ºä»€ä¹ˆéœ€è¦å®šæ—¶ä»»åŠ¡ï¼Ÿ**

```
æ‰‹åŠ¨æ‰§è¡Œï¼š
    æ¯å¤©æ—©ä¸Š9ç‚¹æ‰‹åŠ¨è¿è¡Œæ•°æ®æŠ¥è¡¨
    â†’ å¿˜è®°æ‰§è¡Œï¼Ÿæ•°æ®ç¼ºå¤± âŒ
    â†’ äººåœ¨ä¼‘å‡ï¼Ÿæ²¡äººæ‰§è¡Œ âŒ

å®šæ—¶ä»»åŠ¡ï¼š
    ç¨‹åºæ¯å¤©æ—©ä¸Š9ç‚¹è‡ªåŠ¨è¿è¡Œ
    â†’ å‡†æ—¶æ‰§è¡Œ âœ…
    â†’ å¯é ç¨³å®š âœ…
```

**å†…å®¹**ï¼š
- APSchedulerï¼ˆè½»é‡çº§ï¼‰
- Celery Beatï¼ˆåˆ†å¸ƒå¼ï¼‰
- ä»»åŠ¡æŒä¹…åŒ–
- åˆ†å¸ƒå¼åè°ƒ
- ç›‘æ§å’Œå‘Šè­¦

**å­¦ä¹ ææ–™**ï¼š
- ç¬”è®°ï¼š`notes/07_scheduled_tasks.md`
- ç¬”è®°ï¼š`notes/07a-07e_*`ï¼ˆç³»åˆ—ï¼‰
- ç¤ºä¾‹ï¼š`examples/07_scheduled_tasks/`

**å®Œæˆæ ‡å‡†**ï¼š
- [ ] ç†è§£å®šæ—¶ä»»åŠ¡çš„ä½¿ç”¨åœºæ™¯
- [ ] æŒæ¡APSchedulerçš„åŸºæœ¬ç”¨æ³•
- [ ] ç†è§£Celery Beatçš„æ¶æ„
- [ ] èƒ½å¤Ÿå®ç°åˆ†å¸ƒå¼å®šæ—¶ä»»åŠ¡
- [ ] æŒæ¡ä»»åŠ¡ç›‘æ§å’Œé”™è¯¯å¤„ç†
```

åŒæ—¶æ›´æ–°ç°æœ‰ä¸»é¢˜ç¼–å·ï¼š
- ä¸»é¢˜ 6 â†’ ä¸»é¢˜ 8

**Step 3: Commit**

```bash
git add study/level4/README.md
git commit -m "docs: update Level 4 README with scheduled tasks topic"
```

---

## Task 18: æ›´æ–°é¡¹ç›®ä¾èµ–

**Files:**
- Modify: `requirements.txt`

**Step 1: è¯»å–ç°æœ‰requirements.txt**

```bash
cat requirements.txt
```

**Step 2: æ·»åŠ å®šæ—¶ä»»åŠ¡ä¾èµ–**

åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ ï¼š

```txt
# å®šæ—¶ä»»åŠ¡ï¼ˆLevel 4 - ä¸»é¢˜7ï¼‰
apscheduler>=3.10.0        # APScheduler
celery>=5.3.0              # Celery Beatå’ŒWorker
redis>=5.0.0               # Redisï¼ˆCelery Brokerï¼‰
flower>=2.0.0              # Celeryç›‘æ§ç•Œé¢ï¼ˆå¯é€‰ï¼‰
```

**Step 3: Commit**

```bash
git add requirements.txt
git commit -m "deps: add scheduled tasks dependencies"
```

---

## Task 19: æœ€ç»ˆéªŒè¯

**Step 1: éªŒè¯æ‰€æœ‰æ–‡ä»¶å·²åˆ›å»º**

```bash
# æ£€æŸ¥notesæ–‡ä»¶
ls -la study/level4/notes/07*.md

# æ£€æŸ¥examplesæ–‡ä»¶
ls -la study/level4/examples/07_scheduled_tasks/

# æ£€æŸ¥APSchedulerç¤ºä¾‹
ls -la study/level4/examples/07_scheduled_tasks/apscheduler/

# æ£€æŸ¥Celery Beatç¤ºä¾‹
ls -la study/level4/examples/07_scheduled_tasks/celery_beat/
```

Expected: 18ä¸ªæ–‡ä»¶ï¼ˆ1æ›´æ–° + 17æ–°å»ºï¼‰

**Step 2: éªŒè¯READMEæ›´æ–°**

```bash
grep -A 20 "ä¸»é¢˜ 7" study/level4/README.md
```

Expected: çœ‹åˆ°å®šæ—¶ä»»åŠ¡ä¸»é¢˜å†…å®¹

**Step 3: éªŒè¯ä¾èµ–æ›´æ–°**

```bash
grep "apscheduler\|celery" requirements.txt
```

Expected: çœ‹åˆ°å®šæ—¶ä»»åŠ¡ç›¸å…³ä¾èµ–

**Step 4: åˆ›å»ºæ€»ç»“commit**

```bash
git add .
git commit -m "feat: complete Level 4 scheduled tasks module

- Add 6 notes files (overview + APScheduler + Celery + best practices)
- Add 12 example files (Level 1-4 + APScheduler + Celery)
- Update Level 4 README with new topic
- Update requirements.txt with dependencies

Total: 18 files created/updated

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"
```

---

## éªŒæ”¶æ ‡å‡†

- [ ] 18ä¸ªæ–‡ä»¶å…¨éƒ¨åˆ›å»º/æ›´æ–°
- [ ] æ¯ä¸ªnotesæ–‡ä»¶éƒ½æœ‰è´¹æ›¼æŠ€å·§ç±»æ¯”
- [ ] æ¯ä¸ªnotesæ–‡ä»¶éƒ½æœ‰å°å®éªŒ
- [ ] æ¯ä¸ªexamplesæ–‡ä»¶éƒ½æœ‰è¿è¡Œè¯´æ˜
- [ ] æ‰€æœ‰ä»£ç ç¤ºä¾‹éƒ½å¯ä»¥è¿è¡Œ
- [ ] READMEå·²æ›´æ–°
- [ ] requirements.txtå·²æ›´æ–°

---

**å®æ–½å®Œæˆæ ‡å¿—**: æ‰€æœ‰19ä¸ªä»»åŠ¡å®Œæˆï¼Œæ‰€æœ‰éªŒæ”¶æ ‡å‡†é€šè¿‡
